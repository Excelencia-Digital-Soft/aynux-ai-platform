# üß™ Gu√≠a de Testing y Monitoreo - Aynux Bot

Esta gu√≠a proporciona instrucciones completas para probar y monitorear el comportamiento del bot, incluyendo decisiones de agentes, trazas de LangSmith y visualizaci√≥n gr√°fica del sistema.

## üìã Tabla de Contenidos

1. [Configuraci√≥n Inicial](#configuraci√≥n-inicial)
2. [Herramientas de Testing](#herramientas-de-testing)
3. [LangSmith Integration](#langsmith-integration)
4. [Workflow de Testing](#workflow-de-testing)
5. [Interpretaci√≥n de Resultados](#interpretaci√≥n-de-resultados)
6. [Troubleshooting](#troubleshooting)

---

## üöÄ Configuraci√≥n Inicial

### Prerequisitos

1. **API Key de LangSmith**: Obt√©n tu clave en [smith.langchain.com](https://smith.langchain.com)

2. **Variables de Entorno**: Configura en tu archivo `.env`:

```bash
# LangSmith Configuration (REQUERIDO para tracing)
LANGSMITH_API_KEY=tu_api_key_aqui
LANGSMITH_PROJECT=aynux-production
LANGSMITH_TRACING_ENABLED=true
LANGSMITH_VERBOSE=false
LANGSMITH_SAMPLE_RATE=1.0
LANGSMITH_METRICS_ENABLED=true
```

3. **Dependencias Adicionales**:

```bash
# Instalar dependencias de testing
uv add rich streamlit plotly pandas

# O con pip
pip install rich streamlit plotly pandas
```

### Verificaci√≥n de Configuraci√≥n

Ejecuta el script de verificaci√≥n para asegurar que todo est√° configurado correctamente:

```bash
python tests/test_langsmith_verification.py
```

**Salida Esperada:**
- ‚úÖ LangSmith inicializado correctamente
- ‚úÖ API connection exitosa
- ‚úÖ Trazas creadas exitosamente
- ‚úÖ Servicio de chat operacional

Si alguna verificaci√≥n falla, sigue las instrucciones en pantalla para corregir la configuraci√≥n.

---

## üõ†Ô∏è Herramientas de Testing

### 1. **Verificaci√≥n de LangSmith** (`test_langsmith_verification.py`)

**Prop√≥sito**: Verifica que LangSmith est√° correctamente configurado y funcionando.

**Ejecuci√≥n**:
```bash
python tests/test_langsmith_verification.py
```

**Qu√© Verifica**:
- ‚úÖ Variables de entorno configuradas
- ‚úÖ Conexi√≥n a LangSmith API
- ‚úÖ Creaci√≥n de trazas
- ‚úÖ Procesamiento de conversaciones
- ‚úÖ Almacenamiento de m√©tricas

**Cu√°ndo Usar**:
- Al configurar el proyecto por primera vez
- Despu√©s de cambios en configuraci√≥n
- Para diagnosticar problemas de tracing

---

### 2. **Chat Interactivo** (`test_chat_interactive.py`)

**Prop√≥sito**: Interface de l√≠nea de comandos para probar conversaciones en tiempo real con el mismo backend que WhatsApp.

**Ejecuci√≥n**:
```bash
python tests/test_chat_interactive.py
```

**Caracter√≠sticas**:
- üí¨ Chat en tiempo real con el bot
- ü§ñ Visualizaci√≥n del agente utilizado
- ‚è±Ô∏è M√©tricas de tiempo de procesamiento
- üìä Metadatos de cada respuesta
- üîó Links directos a trazas de LangSmith
- üìú Historial de conversaci√≥n
- üéØ Escenarios predefinidos

**Comandos Disponibles**:

| Comando | Descripci√≥n |
|---------|-------------|
| `<mensaje>` | Enviar mensaje normal |
| `/stream <mensaje>` | Enviar con streaming |
| `/scenarios` | Ver escenarios predefinidos |
| `/run <n√∫mero>` | Ejecutar escenario espec√≠fico |
| `/history` | Ver historial de conversaci√≥n |
| `/traces` | Ver √∫ltimas trazas en LangSmith |
| `/stats` | Mostrar estad√≠sticas de sesi√≥n |
| `/clear` | Reiniciar sesi√≥n |
| `/help` | Mostrar ayuda |
| `/quit` | Salir |

**Ejemplo de Uso**:
```bash
> Hola
Bot (greeting_agent): ¬°Hola! ¬øEn qu√© puedo ayudarte hoy?

> ¬øQu√© laptops tienen?
Bot (product_agent): Tenemos las siguientes laptops disponibles:
1. Dell XPS 15 - $1,299
2. HP Pavilion 14 - $899
...

> /run 1
# Ejecuta el escenario predefinido #1
```

---

### 3. **Dashboard de Monitoreo** (`monitoring_dashboard.py`)

**Prop√≥sito**: Dashboard web interactivo con visualizaciones en tiempo real, m√©tricas de rendimiento y chat de prueba.

**Ejecuci√≥n**:
```bash
streamlit run tests/monitoring_dashboard.py
```

**Caracter√≠sticas Principales**:

#### üìä **Tab 1: Dashboard**
- **M√©tricas Generales**:
  - Total de ejecuciones
  - Tasa de √©xito
  - Latencia promedio y P95
  - Tasa de error

- **Gr√°ficos de Uso**:
  - Distribuci√≥n de uso por agente (pie chart)
  - Ejecuciones por hora (timeline)
  - An√°lisis de errores por tipo

- **Tabla de Ejecuciones Recientes**:
  - √öltimas 20 ejecuciones
  - Estado, latencia, timestamp
  - Link directo a LangSmith

#### üîÄ **Tab 2: Graph Visualization**
- **Visualizaci√≥n del Grafo de Agentes**:
  - Arquitectura del sistema multi-agente
  - Flujo de decisiones y routing
  - Conexiones entre Orchestrator, agentes y Supervisor

- **Explicaci√≥n del Flujo**:
  - Punto de entrada: Orchestrator
  - Agentes especializados por tipo de consulta
  - Supervisor valida y decide si continuar

#### üí¨ **Tab 3: Test Chat**
- **Chat Interactivo en el Dashboard**:
  - Mismo backend que WhatsApp
  - Interfaz visual moderna
  - Metadatos expandibles
  - Trazas autom√°ticas en LangSmith

#### üìñ **Tab 4: Documentaci√≥n**
- Gu√≠a completa del dashboard
- Explicaci√≥n de m√©tricas
- Instrucciones de uso

**Navegaci√≥n del Dashboard**:

1. **Sidebar Izquierdo**:
   - Estado de LangSmith (activo/inactivo)
   - Selector de rango temporal
   - Bot√≥n de refresh

2. **Contenido Principal**:
   - M√©tricas en tarjetas
   - Gr√°ficos interactivos (Plotly)
   - Tablas con datos detallados

**Interpretaci√≥n de Gr√°ficos**:

- **Agent Usage (Pie Chart)**: Muestra qu√© agentes se usan m√°s frecuentemente
- **Performance Timeline**: Identifica picos de tr√°fico y patrones de uso
- **Error Analysis**: Ayuda a identificar problemas recurrentes

---

### 4. **Escenarios de Prueba** (`test_scenarios.py`)

**Prop√≥sito**: Suite de pruebas automatizadas con escenarios predefinidos para validar comportamiento de agentes.

**Ejecuci√≥n**:

```bash
# Ver todos los escenarios disponibles
python tests/test_scenarios.py list

# Ejecutar todos los escenarios
python tests/test_scenarios.py all

# Ejecutar escenario espec√≠fico
python tests/test_scenarios.py run product_query_simple

# Ejecutar por tag
python tests/test_scenarios.py tag products
```

**Escenarios Incluidos**:

| ID | Nombre | Mensajes | Tags |
|----|--------|----------|------|
| `product_query_simple` | Consulta Simple de Productos | 1 | products, simple |
| `product_query_specific` | B√∫squeda Espec√≠fica | 2 | products, search |
| `category_navigation` | Navegaci√≥n por Categor√≠as | 3 | categories, navigation |
| `order_tracking` | Seguimiento de Pedido | 3 | tracking, orders |
| `customer_support` | Soporte al Cliente | 3 | support, returns |
| `invoice_credit_query` | Facturaci√≥n y Cr√©dito | 3 | credit, invoicing |
| `promotions_query` | Consulta de Promociones | 3 | promotions, offers |
| `greeting_farewell` | Saludos y Despedidas | 5 | greeting, farewell |
| `multi_turn_product_purchase` | Compra Multi-Turno | 7 | multi-turn, end-to-end |
| `ambiguous_query` | Consulta Ambigua | 3 | ambiguous, fallback |
| `price_comparison` | Comparaci√≥n de Precios | 3 | products, pricing |
| `specifications_query` | Especificaciones T√©cnicas | 4 | products, technical |
| `availability_stock` | Consulta de Disponibilidad | 3 | products, stock |
| `shipping_delivery` | Env√≠o y Entrega | 4 | shipping, logistics |
| `payment_methods` | M√©todos de Pago | 4 | payment, financing |
| `warranty_returns` | Garant√≠a y Devoluciones | 3 | warranty, policy |

**Validaciones Autom√°ticas**:
- ‚úÖ Agentes utilizados coinciden con los esperados
- ‚úÖ Todas las respuestas se generaron sin errores
- ‚úÖ Tiempos de respuesta dentro de rangos aceptables

**Salida del Test**:
```
üìä TEST EXECUTION SUMMARY
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Total Scenarios: 16
Passed: 14
Failed: 2
Success Rate: 87.5%

‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ ID                        ‚îÉ Name               ‚îÉ Status  ‚îÉ Avg Time (ms)‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ product_query_simple      ‚îÇ Consulta Simple... ‚îÇ ‚úÖ PASS ‚îÇ 1234         ‚îÇ
‚îÇ product_query_specific    ‚îÇ B√∫squeda Espec√≠f...‚îÇ ‚úÖ PASS ‚îÇ 1456         ‚îÇ
...
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üíæ Results saved to: test_results.json
```

---

## üîó LangSmith Integration

### C√≥mo Funciona el Tracing

**LangSmith** es la plataforma de observabilidad de LangChain que rastrea autom√°ticamente todas las ejecuciones del bot.

**Qu√© se Rastrea**:
1. **Cada mensaje del usuario** y la respuesta del bot
2. **Decisiones del Orchestrator**: Qu√© intenci√≥n detect√≥
3. **Agente seleccionado**: Routing decision
4. **Tiempo de procesamiento**: Latencia total y por componente
5. **Errores y excepciones**: Stack traces completos
6. **Contexto de conversaci√≥n**: Session ID, user ID, metadata

**Estructura de Trazas**:

```
Conversaci√≥n (Run)
‚îú‚îÄ‚îÄ Orchestrator (Chain)
‚îÇ   ‚îú‚îÄ‚îÄ Intent Detection (LLM)
‚îÇ   ‚îî‚îÄ‚îÄ Router Decision (Tool)
‚îú‚îÄ‚îÄ ProductAgent (Chain)
‚îÇ   ‚îú‚îÄ‚îÄ Query Processing (LLM)
‚îÇ   ‚îú‚îÄ‚îÄ Database Search (Tool)
‚îÇ   ‚îî‚îÄ‚îÄ Response Generation (LLM)
‚îî‚îÄ‚îÄ Supervisor (Chain)
    ‚îî‚îÄ‚îÄ Validation (LLM)
```

### Ver Trazas en LangSmith

1. **Acceder al Dashboard**:
   - URL: https://smith.langchain.com
   - Login con tu cuenta
   - Selecciona el proyecto configurado en `LANGSMITH_PROJECT`

2. **Filtrar Trazas**:
   - Por fecha/hora
   - Por nombre de agente
   - Por estado (success/error)
   - Por session_id o user_id

3. **Inspeccionar Traza Individual**:
   - Ver √°rbol de ejecuci√≥n completo
   - Inputs y outputs de cada paso
   - Tiempos de ejecuci√≥n
   - Metadata y tags

4. **An√°lisis de Performance**:
   - Latency timeline
   - Token usage (si aplica)
   - Error rates
   - Throughput

### M√©tricas Clave en LangSmith

| M√©trica | Descripci√≥n | Objetivo |
|---------|-------------|----------|
| **Success Rate** | % de ejecuciones sin error | >95% |
| **Avg Latency** | Tiempo promedio de respuesta | <2s |
| **P95 Latency** | Percentil 95 de latencia | <5s |
| **Error Rate** | % de ejecuciones con error | <5% |
| **Agent Distribution** | Uso relativo de cada agente | Balanceado seg√∫n casos de uso |

---

## üìù Workflow de Testing

### Workflow Recomendado para Testing Completo

#### **Fase 1: Configuraci√≥n y Verificaci√≥n** (5-10 min)

```bash
# 1. Verificar configuraci√≥n
python tests/test_langsmith_verification.py

# 2. Revisar estado en LangSmith
# Visitar: https://smith.langchain.com/o/default/projects/p/aynux-production
```

**Resultado Esperado**: ‚úÖ Todas las verificaciones pasan

---

#### **Fase 2: Testing Manual Interactivo** (15-30 min)

```bash
# Iniciar chat interactivo
python tests/test_chat_interactive.py
```

**Qu√© Probar**:
1. **Saludos b√°sicos**: "Hola", "Buenos d√≠as"
2. **Consultas de productos**: "¬øQu√© laptops tienen?"
3. **Navegaci√≥n de categor√≠as**: "Mu√©strame las categor√≠as"
4. **Tracking**: "¬øD√≥nde est√° mi pedido #12345?"
5. **Soporte**: "Mi producto lleg√≥ da√±ado"
6. **Despedidas**: "Gracias, adi√≥s"

**Observar**:
- ‚úÖ Agente correcto seleccionado para cada consulta
- ‚úÖ Respuestas coherentes y relevantes
- ‚úÖ Tiempos de respuesta <3s
- ‚úÖ Metadatos completos

---

#### **Fase 3: Testing Automatizado** (10-20 min)

```bash
# Ejecutar suite completa de escenarios
python tests/test_scenarios.py all
```

**Revisar Resultados**:
- Success rate general
- Escenarios que fallaron (si los hay)
- Tiempos promedio de respuesta
- Archivo `test_results.json` generado

**An√°lisis de Fallos**:
Si alg√∫n escenario falla:
1. Ver detalle del error en consola
2. Revisar qu√© agente se esperaba vs. cu√°l se us√≥
3. Ir a LangSmith para ver la traza completa
4. Identificar la causa ra√≠z

---

#### **Fase 4: Monitoreo con Dashboard** (Tiempo variable)

```bash
# Iniciar dashboard
streamlit run tests/monitoring_dashboard.py
```

**An√°lisis en Dashboard**:

1. **Tab Dashboard**:
   - Revisar m√©tricas generales
   - Identificar agentes m√°s usados
   - Detectar errores recurrentes

2. **Tab Graph Viz**:
   - Entender arquitectura del sistema
   - Verificar flujo de agentes

3. **Tab Test Chat**:
   - Probar casos edge directamente
   - Ver respuestas en tiempo real

4. **Refrescar peri√≥dicamente** para ver tendencias

---

#### **Fase 5: Testing de WhatsApp** (Opcional)

Para probar el comportamiento real en WhatsApp:

1. Configurar webhook de WhatsApp apuntando a tu servidor
2. Enviar mensajes reales desde WhatsApp
3. Observar trazas en LangSmith
4. Comparar comportamiento con chat web

**Diferencia Clave**:
- WhatsApp usa `WebhookService` que procesa mensajes incoming
- Chat web usa API REST directa
- **Mismo backend LangGraph** ‚Üí comportamiento id√©ntico de agentes

---

## üîç Interpretaci√≥n de Resultados

### M√©tricas de √âxito

#### **1. Routing Accuracy** (Precisi√≥n de Enrutamiento)

**Definici√≥n**: ¬øEl Orchestrator selecciona el agente correcto?

**C√≥mo Medir**:
- En test scenarios: ver `agents_match` en resultados
- En LangSmith: revisar decisiones de routing

**Objetivo**: >90% de precisi√≥n

**Qu√© hacer si es bajo**:
- Revisar prompts del `IntentRouter`
- A√±adir ejemplos de intenciones en configuraci√≥n
- Mejorar detecci√≥n de keywords

---

#### **2. Response Quality** (Calidad de Respuestas)

**Definici√≥n**: ¬øLas respuestas son √∫tiles, coherentes y correctas?

**C√≥mo Medir**:
- Revisi√≥n manual de respuestas
- Feedback de usuarios
- LangSmith evaluators (opcional)

**Criterios**:
- ‚úÖ Responde la pregunta del usuario
- ‚úÖ Tono apropiado (profesional, amigable)
- ‚úÖ Sin alucinaciones o informaci√≥n incorrecta
- ‚úÖ Contexto conversacional mantenido

---

#### **3. Performance** (Rendimiento)

**M√©tricas Clave**:

| M√©trica | Objetivo | Cr√≠tico |
|---------|----------|---------|
| Avg Latency | <2s | <5s |
| P95 Latency | <5s | <10s |
| Error Rate | <5% | <10% |
| Success Rate | >95% | >90% |

**Optimizaciones si es lento**:
- Revisar queries a base de datos
- Optimizar embeddings y b√∫squeda vectorial
- Cachear respuestas frecuentes en Redis
- Reducir complejidad de prompts

---

#### **4. Error Handling** (Manejo de Errores)

**Tipos de Errores Comunes**:

1. **Database Errors**: Conexi√≥n a PostgreSQL falla
   - Soluci√≥n: Verificar configuraci√≥n de DB, pool size

2. **AI Model Errors**: Ollama no responde
   - Soluci√≥n: Verificar servicio Ollama activo, modelo descargado

3. **Integration Errors**: DUX API, WhatsApp API fallan
   - Soluci√≥n: Implementar fallbacks y reintentos

4. **Validation Errors**: Input del usuario inv√°lido
   - Soluci√≥n: Mejorar validaci√≥n y mensajes de error

**En LangSmith**: Filtrar por `error=true` para ver trazas con errores

---

### An√°lisis de Conversaciones Multi-Turno

**Qu√© Verificar**:
- ‚úÖ Contexto mantenido entre mensajes
- ‚úÖ Referencias a mensajes anteriores funcionan
- ‚úÖ Estado de conversaci√≥n persiste correctamente
- ‚úÖ Transiciones entre agentes son fluidas

**Ejemplo de Conversaci√≥n Multi-Turno Exitosa**:
```
User: Hola                          ‚Üí GreetingAgent
User: ¬øQu√© laptops tienen?          ‚Üí ProductAgent (lista productos)
User: ¬øCu√°l es la m√°s barata?       ‚Üí ProductAgent (filtra por precio, mantiene contexto)
User: ¬øY la m√°s potente?            ‚Üí ProductAgent (filtra por specs, mantiene contexto)
User: Quiero comprar la Dell XPS 15 ‚Üí ProductAgent (procesa selecci√≥n)
User: ¬øC√≥mo puedo pagar?            ‚Üí SupportAgent (info de pago)
User: Gracias                       ‚Üí FarewellAgent
```

**Red Flags**:
- ‚ùå Agente no recuerda selecci√≥n anterior
- ‚ùå Contexto se pierde despu√©s de 3-4 mensajes
- ‚ùå Respuestas contradictorias

---

## üêõ Troubleshooting

### Problema: LangSmith no muestra trazas

**S√≠ntomas**:
- Dashboard vac√≠o en LangSmith
- No hay trazas despu√©s de conversaciones

**Diagn√≥stico**:
```bash
python tests/test_langsmith_verification.py
```

**Soluciones**:

1. **API Key Incorrecta**:
   ```bash
   # Verificar .env
   cat .env | grep LANGSMITH_API_KEY

   # Deber√≠a mostrar tu key real, no 'your_api_key_here'
   ```

2. **Tracing Deshabilitado**:
   ```bash
   # En .env
   LANGSMITH_TRACING_ENABLED=true  # Debe ser 'true'
   ```

3. **Proyecto Incorrecto**:
   ```bash
   # Verificar que el proyecto existe en LangSmith
   # Ir a: https://smith.langchain.com
   # El nombre debe coincidir exactamente con LANGSMITH_PROJECT
   ```

4. **Variables de Entorno no Cargadas**:
   ```bash
   # Reiniciar servicio despu√©s de cambios en .env
   # O usar python-dotenv para cargar autom√°ticamente
   ```

---

### Problema: Agentes seleccionados incorrectamente

**S√≠ntomas**:
- ProductAgent usado cuando deber√≠a ser SupportAgent
- FallbackAgent usado con frecuencia alta

**Diagn√≥stico**:
- Revisar trazas en LangSmith ‚Üí ver decisi√≥n de `IntentRouter`
- Ver prompt y reasoning del Orchestrator

**Soluciones**:

1. **Mejorar Detecci√≥n de Intenciones**:
   - Editar `app/agents/intelligence/intent_router.py`
   - A√±adir keywords espec√≠ficas
   - Mejorar ejemplos de intenciones

2. **Ajustar Prompts del Orchestrator**:
   - M√°s ejemplos de routing correcto
   - Instrucciones m√°s claras

3. **Revisar Configuraci√≥n de Agentes**:
   - Verificar que cada agente tiene descripci√≥n clara
   - Asegurar que capabilities est√°n bien definidas

---

### Problema: Respuestas lentas (>5s)

**S√≠ntomas**:
- P95 latency >10s
- Usuarios reportan lentitud

**Diagn√≥stico**:
- En LangSmith: ver timeline de cada step
- Identificar componente m√°s lento

**Soluciones por Componente**:

1. **Ollama (LLM)**:
   - Usar modelo m√°s peque√±o: `llama3.2:1b` vs `deepseek-r1:7b`
   - Verificar GPU disponible
   - Reducir longitud de context window

2. **Database Queries**:
   - A√±adir √≠ndices en PostgreSQL
   - Optimizar queries complejas
   - Implementar caching en Redis

3. **Vector Search**:
   - Reducir n√∫mero de resultados retornados
   - Verificar que pgvector est√© optimizado correctamente
   - Pre-calcular embeddings

4. **Network**:
   - Verificar latencia a APIs externas (DUX, WhatsApp)
   - Implementar timeouts apropiados
   - Usar async/await correctamente

---

### Problema: Errores en Escenarios Automatizados

**S√≠ntomas**:
- Test scenarios fallan consistentemente
- `agents_match = False`

**Diagn√≥stico**:
```bash
# Ver detalle de fallo
python tests/test_scenarios.py run <scenario_id>

# Revisar archivo de resultados
cat tests/test_results.json | jq '.[] | select(.success == false)'
```

**Soluciones**:

1. **Actualizar Expectativas**:
   - Si el comportamiento cambi√≥ intencionalmente
   - Editar `expected_agents` en `test_scenarios.py`

2. **Mejorar Escenario**:
   - Mensajes m√°s claros
   - Contexto adicional en metadata

3. **Debuggear en Chat Interactivo**:
   ```bash
   # Probar manualmente el mismo escenario
   python tests/test_chat_interactive.py
   # Enviar los mismos mensajes y ver qu√© pasa
   ```

---

### Problema: Dashboard de Streamlit no carga

**S√≠ntomas**:
- Error al ejecutar `streamlit run monitoring_dashboard.py`
- Dashboard se bloquea

**Soluciones**:

1. **Dependencias Faltantes**:
   ```bash
   uv add streamlit plotly pandas rich
   ```

2. **Puerto en Uso**:
   ```bash
   # Streamlit usa puerto 8501 por defecto
   # Si est√° en uso, especificar otro
   streamlit run tests/monitoring_dashboard.py --server.port 8502
   ```

3. **Error de Inicializaci√≥n del Servicio**:
   - Verificar que PostgreSQL est√° corriendo
   - Verificar que Ollama est√° activo
   - Revisar logs en consola

---

## üìö Recursos Adicionales

### Documentaci√≥n de Referencia

- **LangSmith Docs**: https://docs.smith.langchain.com
- **LangGraph Docs**: https://langchain-ai.github.io/langgraph/
- **Streamlit Docs**: https://docs.streamlit.io

### Configuraci√≥n Avanzada

#### Custom Evaluators en LangSmith

Para evaluaci√≥n autom√°tica de calidad:

```python
# app/evaluation/langsmith_evaluators.py
from langsmith.evaluation import LangSmithRunEvaluator

def custom_evaluator(run, example):
    # L√≥gica de evaluaci√≥n
    return {"score": 0.9, "reasoning": "Good response"}

evaluators = [LangSmithRunEvaluator(custom_evaluator)]
```

#### A/B Testing con LangSmith

Comparar versiones de prompts o modelos:

```python
# Experiment 1: Prompt V1
# Experiment 2: Prompt V2
# LangSmith mostrar√° comparaci√≥n de m√©tricas
```

---

## üéØ Mejores Pr√°cticas

### Testing Regular

1. **Daily**: Ejecutar `test_langsmith_verification.py`
2. **Weekly**: Suite completa de escenarios automatizados
3. **Monthly**: Revisi√≥n profunda de m√©tricas en LangSmith
4. **Continuous**: Dashboard de Streamlit abierto durante desarrollo

### Organizaci√≥n de Tests

- Crear escenarios para cada nuevo feature
- Mantener tests actualizados con cambios en agentes
- Documentar casos edge descubiertos

### Monitoreo en Producci√≥n

- Configurar alertas en LangSmith para error rate >5%
- Revisar m√©tricas semanalmente
- Mantener hist√≥rico de trazas para an√°lisis

---

## üÜò Soporte

Si encuentras problemas no cubiertos en esta gu√≠a:

1. **Revisar Logs**: `app/main.py` tiene logging detallado
2. **LangSmith Traces**: Ver errores completos con stack traces
3. **GitHub Issues**: Reportar bugs en el repositorio
4. **Documentaci√≥n**: Revisar docs/ para detalles de arquitectura

---

**¬°Happy Testing! üéâ**
