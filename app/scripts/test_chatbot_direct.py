#!/usr/bin/env python3
"""
Script para probar el servicio de chatbot directamente sin WhatsApp
"""

import asyncio
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List

# AÃ±adir el directorio raÃ­z al path
sys.path.append(str(Path(__file__).parent.parent.parent))

from app.models.message import Contact, TextMessage, WhatsAppMessage
from app.services.chatbot_service import ChatbotService
from app.services.langgraph_chatbot_service import LangGraphChatbotService


class ChatbotTester:
    """Clase para probar servicios de chatbot directamente"""

    def __init__(self, use_langgraph: bool = True):
        self.use_langgraph = use_langgraph
        self.service = None
        self.conversation_history = []
        self.user_number = "5491234567890"  # NÃºmero de prueba
        self.user_name = "Usuario de Prueba"

        # Configurar logging
        logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        self.logger = logging.getLogger(__name__)

    async def initialize(self):
        """Inicializa el servicio de chatbot"""
        print(f"ğŸ¤– Inicializando servicio {'LangGraph' if self.use_langgraph else 'Traditional'}...")

        try:
            if self.use_langgraph:
                self.service = LangGraphChatbotService()
                await self.service.initialize()
                print("âœ… Servicio LangGraph inicializado")
            else:
                self.service = ChatbotService()
                print("âœ… Servicio Traditional inicializado")

            return True

        except Exception as e:
            print(f"âŒ Error inicializando servicio: {e}")
            return False

    def create_test_message(self, text: str) -> tuple:
        """Crea un mensaje de prueba"""
        message = WhatsAppMessage(
            from_=self.user_number,
            id=f"test_msg_{len(self.conversation_history):03d}",
            type="text",
            timestamp=str(int(datetime.now().timestamp())),
            text=TextMessage(body=text),
        )

        contact = Contact(wa_id=self.user_number, profile={"name": self.user_name})

        return message, contact

    async def send_message(self, text: str) -> Dict:
        """EnvÃ­a un mensaje al chatbot y obtiene la respuesta"""
        print(f"\nğŸ‘¤ Usuario: {text}")

        # Crear mensaje de prueba
        message, contact = self.create_test_message(text)

        # Procesar mensaje
        try:
            start_time = datetime.now()
            result = await self.service.process_webhook_message(message, contact)
            end_time = datetime.now()

            response_time = (end_time - start_time).total_seconds()

            print(f"ğŸ¤– Bot ({response_time:.2f}s): {result.message}")

            # Guardar en historial
            self.conversation_history.append(
                {
                    "timestamp": start_time.isoformat(),
                    "user_message": text,
                    "bot_response": result.message,
                    "status": result.status,
                    "response_time": response_time,
                }
            )

            return {
                "success": result.status == "success",
                "response": result.message,
                "response_time": response_time,
                "timestamp": start_time.isoformat(),
            }

        except Exception as e:
            print(f"âŒ Error procesando mensaje: {e}")
            return {"success": False, "error": str(e), "timestamp": datetime.now().isoformat()}

    async def run_conversation_test(self, messages: List[str]):
        """Ejecuta una conversaciÃ³n de prueba completa"""
        print("ğŸ—£ï¸  Iniciando conversaciÃ³n de prueba...")

        results = []
        for i, message in enumerate(messages, 1):
            print(f"\n--- Mensaje {i}/{len(messages)} ---")
            result = await self.send_message(message)
            results.append(result)

            # Pausa entre mensajes
            await asyncio.sleep(1)

        return results

    def generate_conversation_summary(self) -> Dict:
        """Genera un resumen de la conversaciÃ³n"""
        if not self.conversation_history:
            return {"error": "No conversation history"}

        total_messages = len(self.conversation_history)
        successful_responses = sum(1 for msg in self.conversation_history if msg.get("status") == "success")
        avg_response_time = sum(msg.get("response_time", 0) for msg in self.conversation_history) / total_messages

        return {
            "total_messages": total_messages,
            "successful_responses": successful_responses,
            "success_rate": (successful_responses / total_messages) * 100,
            "average_response_time": avg_response_time,
            "conversation_duration": (
                datetime.fromisoformat(self.conversation_history[-1]["timestamp"])
                - datetime.fromisoformat(self.conversation_history[0]["timestamp"])
            ).total_seconds(),
        }

    def save_conversation_log(self, filename: str = None):
        """Guarda el log de la conversaciÃ³n"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            service_type = "langgraph" if self.use_langgraph else "traditional"
            
            # Create logs directory in the project root
            logs_dir = Path(__file__).parent.parent.parent / "logs"
            logs_dir.mkdir(exist_ok=True)
            
            filename = logs_dir / f"conversation_test_{service_type}_{timestamp}.json"

        log_data = {
            "test_info": {
                "service_type": "langgraph" if self.use_langgraph else "traditional",
                "user_number": self.user_number,
                "user_name": self.user_name,
                "test_timestamp": datetime.now().isoformat(),
            },
            "conversation_history": self.conversation_history,
            "summary": self.generate_conversation_summary(),
        }

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ Log guardado en: {filename}")
        return str(filename)

    async def cleanup(self):
        """Limpia recursos"""
        if self.service and hasattr(self.service, "cleanup"):
            await self.service.cleanup()
        print("ğŸ§¹ Recursos limpiados")


# Conversaciones de prueba predefinidas
TEST_CONVERSATIONS = {
    "saludo_basico": ["Hola", "Â¿QuÃ© productos tienen?", "Gracias, hasta luego"],
    "consulta_laptops": [
        "Hola, buenos dÃ­as",
        "Necesito una laptop para gaming",
        "Â¿CuÃ¡les tienen en stock?",
        "Â¿CuÃ¡l es el precio de la mÃ¡s econÃ³mica?",
        "Â¿Tienen garantÃ­a?",
        "Perfecto, muchas gracias",
    ],
    "consulta_componentes": [
        "Hola",
        "Estoy armando una PC gamer",
        "Â¿QuÃ© procesadores Ryzen tienen?",
        "Â¿Y tarjetas de video RTX?",
        "Â¿CuÃ¡l es el precio del combo mÃ¡s recomendado?",
        "Â¿Hacen descuento por cantidad?",
        "Gracias por la informaciÃ³n",
    ],
    "consulta_stock": [
        "Buenos dÃ­as",
        "Â¿Tienen stock de laptops Asus?",
        "Â¿Y de la marca HP?",
        "Â¿CuÃ¡ndo les llega mercaderÃ­a nueva?",
        "Perfecto, los contacto mÃ¡s tarde",
    ],
    "conversacion_compleja": [
        "Hola, Â¿cÃ³mo estÃ¡n?",
        "Necesito equipar una oficina con 5 computadoras",
        "Debe ser para trabajo de oficina, nada muy exigente",
        "Â¿CuÃ¡l serÃ­a el presupuesto aproximado?",
        "Â¿Incluye monitor, teclado y mouse?",
        "Â¿QuÃ© garantÃ­a tienen?",
        "Â¿Hacen instalaciÃ³n a domicilio?",
        "Â¿Aceptan pago en cuotas?",
        "Perfecto, me paso por el local maÃ±ana",
        "Muchas gracias por toda la informaciÃ³n",
    ],
}


async def main():
    """FunciÃ³n principal"""
    print("ğŸ§ª CHATBOT DIRECT TESTER")
    print("=" * 50)

    # MenÃº de opciones
    print("Opciones disponibles:")
    print("1. ConversaciÃ³n interactiva")
    print("2. Test con conversaciÃ³n predefinida")
    print("3. Test de performance (mÃºltiples conversaciones)")
    print("4. Comparar servicios (LangGraph vs Traditional)")

    choice = input("\nSelecciona una opciÃ³n (1-4): ").strip()

    if choice == "1":
        await interactive_conversation()
    elif choice == "2":
        await predefined_conversation_test()
    elif choice == "3":
        await performance_test()
    elif choice == "4":
        await compare_services()
    else:
        print("âŒ OpciÃ³n no vÃ¡lida")


async def interactive_conversation():
    """ConversaciÃ³n interactiva con el usuario"""
    use_langgraph = input("Â¿Usar LangGraph? (y/n): ").lower().startswith("y")

    tester = ChatbotTester(use_langgraph=use_langgraph)

    if not await tester.initialize():
        return

    print("\nğŸ’¬ ConversaciÃ³n interactiva iniciada")
    print("Escribe 'quit' para terminar")

    try:
        while True:
            user_input = input("\nğŸ‘¤ Tu mensaje: ").strip()

            if user_input.lower() in ["quit", "exit", "salir"]:
                break

            if user_input:
                await tester.send_message(user_input)

        # Mostrar resumen
        summary = tester.generate_conversation_summary()
        print("\nğŸ“Š Resumen de la conversaciÃ³n:")
        print(f"  â€¢ Mensajes totales: {summary['total_messages']}")
        print(f"  â€¢ Tasa de Ã©xito: {summary['success_rate']:.1f}%")
        print(f"  â€¢ Tiempo promedio: {summary['average_response_time']:.2f}s")

        # Guardar log
        filename = tester.save_conversation_log()
        print(f"  â€¢ Log guardado: {filename}")

    finally:
        await tester.cleanup()


async def predefined_conversation_test():
    """Test con conversaciones predefinidas"""
    print("\nConversaciones disponibles:")
    for i, name in enumerate(TEST_CONVERSATIONS.keys(), 1):
        print(f"{i}. {name}")

    choice = input("\nSelecciona una conversaciÃ³n (nÃºmero): ").strip()

    try:
        conversation_name = list(TEST_CONVERSATIONS.keys())[int(choice) - 1]
        messages = TEST_CONVERSATIONS[conversation_name]
    except (ValueError, IndexError):
        print("âŒ SelecciÃ³n no vÃ¡lida")
        return

    use_langgraph = input("Â¿Usar LangGraph? (y/n): ").lower().startswith("y")

    tester = ChatbotTester(use_langgraph=use_langgraph)

    if not await tester.initialize():
        return

    print(f"\nğŸ­ Ejecutando conversaciÃ³n: {conversation_name}")

    try:
        results = await tester.run_conversation_test(messages)

        # Mostrar resumen
        summary = tester.generate_conversation_summary()
        print("\nğŸ“Š Resultados del test:")
        print(f"  â€¢ ConversaciÃ³n: {conversation_name}")
        print(f"  â€¢ Servicio: {'LangGraph' if use_langgraph else 'Traditional'}")
        print(f"  â€¢ Mensajes procesados: {summary['total_messages']}")
        print(f"  â€¢ Tasa de Ã©xito: {summary['success_rate']:.1f}%")
        print(f"  â€¢ Tiempo promedio: {summary['average_response_time']:.2f}s")
        print(f"  â€¢ DuraciÃ³n total: {summary['conversation_duration']:.2f}s")

        # Guardar log
        filename = tester.save_conversation_log()

    finally:
        await tester.cleanup()


async def performance_test():
    """Test de performance con mÃºltiples conversaciones"""
    use_langgraph = input("Â¿Usar LangGraph? (y/n): ").lower().startswith("y")

    print(f"\nâš¡ Test de performance con servicio {'LangGraph' if use_langgraph else 'Traditional'}")

    all_results = []

    for conversation_name, messages in TEST_CONVERSATIONS.items():
        print(f"\nğŸ”„ Procesando: {conversation_name}")

        tester = ChatbotTester(use_langgraph=use_langgraph)

        if not await tester.initialize():
            continue

        try:
            results = await tester.run_conversation_test(messages)
            summary = tester.generate_conversation_summary()

            all_results.append(
                {"conversation": conversation_name, "summary": summary, "details": tester.conversation_history}
            )

            print(
                f"  âœ… Completado - {summary['success_rate']:.1f}% Ã©xito, {summary['average_response_time']:.2f}s promedio"
            )

        finally:
            await tester.cleanup()

    # Resumen general
    print("\nğŸ“ˆ RESUMEN GENERAL DE PERFORMANCE")
    print("=" * 50)

    total_messages = sum(r["summary"]["total_messages"] for r in all_results)
    avg_success_rate = sum(r["summary"]["success_rate"] for r in all_results) / len(all_results)
    avg_response_time = sum(r["summary"]["average_response_time"] for r in all_results) / len(all_results)

    print(f"ğŸ¯ Conversaciones procesadas: {len(all_results)}")
    print(f"ğŸ“ Mensajes totales: {total_messages}")
    print(f"âœ… Tasa de Ã©xito promedio: {avg_success_rate:.1f}%")
    print(f"âš¡ Tiempo de respuesta promedio: {avg_response_time:.2f}s")

    # Guardar resultados
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    service_type = "langgraph" if use_langgraph else "traditional"
    
    # Create logs directory in the project root
    logs_dir = Path(__file__).parent.parent.parent / "logs"
    logs_dir.mkdir(exist_ok=True)
    
    filename = logs_dir / f"performance_test_{service_type}_{timestamp}.json"

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(
            {
                "test_info": {
                    "service_type": service_type,
                    "test_timestamp": datetime.now().isoformat(),
                    "conversations_count": len(all_results),
                    "total_messages": total_messages,
                },
                "summary": {"average_success_rate": avg_success_rate, "average_response_time": avg_response_time},
                "detailed_results": all_results,
            },
            f,
            indent=2,
            ensure_ascii=False,
        )

    print(f"ğŸ’¾ Resultados guardados en: {filename}")


async def compare_services():
    """Compara el rendimiento entre servicios"""
    print("\nğŸ” COMPARACIÃ“N DE SERVICIOS")
    print("=" * 50)

    # Seleccionar conversaciÃ³n para comparar
    print("Conversaciones disponibles:")
    for i, name in enumerate(TEST_CONVERSATIONS.keys(), 1):
        print(f"{i}. {name}")

    choice = input("\nSelecciona una conversaciÃ³n para comparar (nÃºmero): ").strip()

    try:
        conversation_name = list(TEST_CONVERSATIONS.keys())[int(choice) - 1]
        messages = TEST_CONVERSATIONS[conversation_name]
    except (ValueError, IndexError):
        print("âŒ SelecciÃ³n no vÃ¡lida")
        return

    results = {}

    # Test con ambos servicios
    for service_name, use_langgraph in [("Traditional", False), ("LangGraph", True)]:
        print(f"\nğŸ§ª Probando con {service_name}...")

        tester = ChatbotTester(use_langgraph=use_langgraph)

        if not await tester.initialize():
            print(f"âŒ No se pudo inicializar {service_name}")
            continue

        try:
            await tester.run_conversation_test(messages)
            summary = tester.generate_conversation_summary()
            results[service_name] = summary

            print(
                f"  âœ… {service_name}: {summary['success_rate']:.1f}% Ã©xito, {summary['average_response_time']:.2f}s promedio"
            )

        finally:
            await tester.cleanup()

    # Mostrar comparaciÃ³n
    if len(results) == 2:
        print("\nğŸ“Š COMPARACIÃ“N DETALLADA")
        print("=" * 50)

        for metric in ["total_messages", "success_rate", "average_response_time", "conversation_duration"]:
            print(f"\n{metric.replace('_', ' ').title()}:")
            for service, data in results.items():
                value = data[metric]
                if "rate" in metric:
                    print(f"  {service}: {value:.1f}%")
                elif "time" in metric:
                    print(f"  {service}: {value:.2f}s")
                else:
                    print(f"  {service}: {value}")

        # Determinar ganador
        langgraph_time = results.get("LangGraph", {}).get("average_response_time", float("inf"))
        traditional_time = results.get("Traditional", {}).get("average_response_time", float("inf"))

        if langgraph_time < traditional_time:
            print(f"\nğŸ† LangGraph es {((traditional_time / langgraph_time - 1) * 100):.1f}% mÃ¡s rÃ¡pido")
        else:
            print(f"\nğŸ† Traditional es {((langgraph_time / traditional_time - 1) * 100):.1f}% mÃ¡s rÃ¡pido")

        # Guardar comparaciÃ³n
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Create logs directory in the project root
        logs_dir = Path(__file__).parent.parent.parent / "logs"
        logs_dir.mkdir(exist_ok=True)
        
        filename = logs_dir / f"service_comparison_{timestamp}.json"

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "comparison_info": {"conversation": conversation_name, "timestamp": datetime.now().isoformat()},
                    "results": results,
                },
                f,
                indent=2,
                ensure_ascii=False,
            )

        print(f"ğŸ’¾ ComparaciÃ³n guardada en: {filename}")


if __name__ == "__main__":
    asyncio.run(main())
