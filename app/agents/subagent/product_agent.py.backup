"""
Agente especializado en consultas de productos
"""

import json
import logging
import os
from typing import Any, Dict, List, Optional, Tuple, cast

from langchain_core.documents import Document

from app.config.settings import get_settings
from app.services.whatsapp_catalog_service import WhatsAppCatalogService
from app.utils import extract_json_from_text

from ..integrations.chroma_integration import ChromaDBIntegration
from ..integrations.ollama_integration import OllamaIntegration
from ..integrations.pgvector_integration import PgVectorIntegration
from ..product.intent_analyzer import IntentAnalyzer
from ..tools.product_tool import ProductTool
from ..utils.tracing import trace_async_method
from .base_agent import BaseAgent

logger = logging.getLogger(__name__)


class ProductAgent(BaseAgent):
    """Agente especializado en consultas especÃ­ficas de productos y stock"""

    def __init__(self, ollama=None, postgres=None, config: Optional[Dict[str, Any]] = None):
        super().__init__("product_agent", config or {}, ollama=ollama, postgres=postgres)

        settings = get_settings()

        # ConfiguraciÃ³n especÃ­fica del agente
        self.max_products_shown = self.config.get("max_products_shown", 10)
        self.show_stock = self.config.get("show_stock", True)
        self.show_prices = self.config.get("show_prices", True)
        self.enable_recommendations = self.config.get("enable_recommendations", True)

        # Initialize tools
        self.product_tool = ProductTool()
        self.ollama = ollama or OllamaIntegration()

        # Initialize pgvector integration (primary search method)
        self.pgvector = PgVectorIntegration(ollama=self.ollama)
        self.use_pgvector = getattr(settings, "USE_PGVECTOR", True)  # Feature flag

        # Initialize ChromaDB integration for fallback/legacy support
        all_products_path = os.path.join(settings.OLLAMA_API_CHROMADB, "products", "all_products")
        self.chroma = ChromaDBIntegration(all_products_path)
        self.chroma_collection = "products_all_products"

        # Configure search thresholds
        self.min_results = 2
        self.pgvector_similarity_threshold = 0.7  # Higher precision for pgvector
        self.chroma_similarity_threshold = 0.5  # Lower recall for ChromaDB fallback

        # Search strategy: "pgvector_primary" | "chroma_primary" | "hybrid"
        self.search_strategy = getattr(settings, "PRODUCT_SEARCH_STRATEGY", "pgvector_primary")

        # Always use PostgreSQL database as ultimate fallback
        self.data_source = "database"

        # Initialize WhatsApp Catalog Service for interactive product displays
        self.enable_whatsapp_catalog = self.config.get("enable_whatsapp_catalog", True)
        self.catalog_service = None
        if self.enable_whatsapp_catalog:
            try:
                self.catalog_service = WhatsAppCatalogService()
                self.logger.info("WhatsApp Catalog Service initialized")
            except Exception as e:
                self.logger.warning(f"WhatsApp Catalog Service not available: {str(e)}")
                self.catalog_service = None

        # SOLID Refactoring: Initialize IntentAnalyzer (Phase 2) behind feature flag
        self.use_refactored_intent_analyzer = getattr(settings, "USE_REFACTORED_INTENT_ANALYZER", False)
        self.intent_analyzer = None
        if self.use_refactored_intent_analyzer:
            self.intent_analyzer = IntentAnalyzer(ollama=self.ollama, temperature=0.3)
            self.logger.info("Using refactored IntentAnalyzer (Phase 2)")

    @trace_async_method(
        name="product_agent_process",
        run_type="chain",
        metadata={"agent_type": "product", "data_source": "database"},
        extract_state=True,
    )
    async def _process_internal(self, message: str, state_dict: Dict[str, Any]) -> Dict[str, Any]:
        """
        Procesa consultas de productos usando AI y base de datos.

        Args:
            message: Mensaje del usuario
            state_dict: Estado actual como diccionario

        Returns:
            Diccionario con actualizaciones para el estado
        """
        try:
            # Analyze user intent using AI (refactored or legacy)
            if self.use_refactored_intent_analyzer and self.intent_analyzer:
                # Use refactored IntentAnalyzer (Phase 2)
                intent_obj = await self.intent_analyzer.analyze_intent(message)
                intent_analysis = intent_obj.to_dict()
                self.logger.debug(f"Using refactored IntentAnalyzer: {intent_analysis['intent']}")
            else:
                # Use legacy intent analysis
                intent_analysis = await self._analyze_user_intent(message)
                self.logger.debug(f"Using legacy intent analysis: {intent_analysis['intent']}")

            products = []
            response_text = ""
            source = "unknown"
            search_metadata = {}

            # Step 1: Try pgvector semantic search first (if enabled)
            if self.use_pgvector and self.search_strategy in ["pgvector_primary", "hybrid"]:
                self.logger.debug("Attempting pgvector semantic search...")
                pgvector_data = await self._query_products_from_pgvector(message, intent_analysis)

                pgvector_products = pgvector_data.get("products", [])
                pgvector_success = pgvector_data.get("success", False)

                if pgvector_success and len(pgvector_products) >= self.min_results:
                    # pgvector found sufficient results - use them
                    products = pgvector_products
                    source = "pgvector"
                    search_metadata = pgvector_data
                    self.logger.info(f"Using pgvector results: {len(products)} products found")

            # Step 2: Fallback to ChromaDB if pgvector failed or not enough results
            if not products and self.search_strategy in ["chroma_primary", "hybrid"]:
                self.logger.debug("Attempting ChromaDB semantic search...")
                chroma_data = await self._query_products_from_chroma(message, intent_analysis)

                chroma_products = chroma_data.get("products", [])
                chroma_success = chroma_data.get("success", False)

                if chroma_success and len(chroma_products) >= self.min_results:
                    # ChromaDB found sufficient results - use them
                    products = chroma_products
                    source = "chroma"
                    search_metadata = chroma_data
                    self.logger.info(f"Using ChromaDB results: {len(products)} products found")

            # Step 3: Ultimate fallback to database search
            if not products:
                self.logger.info(
                    f"Vector search insufficient results, falling back to database search..."
                )

                products_data = await self._get_products_from_db(intent_analysis)

                if not products_data["success"]:
                    raise Exception(f"Error fetching products: {products_data.get('error', 'Unknown error')}")

                products = products_data.get("products", [])
                source = "database"
                search_metadata = products_data

            # Generate response based on products found
            if products:
                # Try WhatsApp catalog first if available
                response_text, catalog_sent = await self._try_catalog_response(
                    message, intent_analysis, products, state_dict
                )

                # If catalog not sent, generate traditional AI response
                if not catalog_sent:
                    if source == "pgvector":
                        response_text = await self._generate_pgvector_ai_response(
                            products, message, intent_analysis, search_metadata
                        )
                    elif source == "chroma":
                        response_text = await self._generate_chroma_ai_response(
                            products, message, intent_analysis, search_metadata
                        )
                    else:
                        response_text = await self._generate_ai_response(products, message, intent_analysis)
            else:
                # No products found
                response_text = await self._generate_no_results_response(message, intent_analysis)

            return {
                "messages": [{"role": "assistant", "content": response_text}],
                "current_agent": self.name,
                "agent_history": [self.name],
                "retrieved_data": {
                    "products": products,
                    "intent": intent_analysis,
                    "source": source,
                    "search_metadata": search_metadata,
                },
                "is_complete": True,
            }

        except Exception as e:
            logger.error(f"Error in product agent: {str(e)}")

            error_response = "Disculpa, tuve un problema consultando los productos. Â¿PodrÃ­as reformular tu pregunta?"

            return {
                "messages": [{"role": "assistant", "content": error_response}],
                "error_count": state_dict.get("error_count", 0) + 1,
                "current_agent": self.name,
            }

    async def _analyze_user_intent(self, message: str) -> Dict[str, Any]:
        """Analyze user intent using AI."""
        prompt = f"""# USER MESSAGE
"{message}"

# INSTRUCTIONS
You are analyzing a user's product inquiry for an e-commerce system. Extract the user's intent and respond with JSON:

{{
  "intent": "show_general_catalog|search_specific_products|search_by_category|search_by_brand|search_by_price
    |get_product_details",
  "search_terms": ["specific", "product", "terms"],
  "category": "category_name_or_null",
  "brand": "brand_name_or_null", 
  "price_min": float_or_null,
  "price_max": float_or_null,
  "specific_product": "exact_product_name_or_null",
  "wants_stock_info": boolean,
  "wants_featured": boolean,
  "wants_sale": boolean,
  "action_needed": "show_featured|search_products|search_category|search_brand|search_price"
}}

INTENT ANALYSIS:
- show_general_catalog: User asks what products are available, general catalog inquiry ("what products do you have",
    "show me your products")
- search_specific_products: User wants specific products ("show me laptops", "I need a phone")
- search_by_category: User mentions a specific category
- search_by_brand: User mentions a specific brand  
- search_by_price: User mentions price range
- get_product_details: User asks about a specific product

For search_terms, only include meaningful product-related words, not filler words."""

        default_intent = {
            "intent": "search_general",
            "search_terms": message.split(),
            "category": None,
            "brand": None,
            "price_min": None,
            "price_max": None,
            "specific_product": None,
            "wants_stock_info": False,
            "wants_featured": False,
            "wants_sale": False,
        }

        try:
            llm = self.ollama.get_llm(temperature=0.3)
            response = await llm.ainvoke(prompt)

            # Extract JSON from response using the utility function
            required_keys = ["intent"]  # Minimum required key
            extracted_json = extract_json_from_text(
                response.content, default=default_intent, required_keys=required_keys
            )

            # Ensure all expected keys are present with defaults
            if extracted_json and isinstance(extracted_json, dict):
                for key, value in default_intent.items():
                    if key not in extracted_json:
                        extracted_json[key] = value

                return extracted_json
            else:
                return default_intent

        except Exception as e:
            logger.error(f"Error analyzing product intent: {str(e)}")
            return default_intent

    async def _get_products_from_db(self, intent_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Fetch products from database based on AI intent analysis."""
        intent = intent_analysis.get("intent", "show_general_catalog")
        action_needed = intent_analysis.get("action_needed", "show_featured")

        # Use AI-determined action to decide what to fetch
        if action_needed == "show_featured" or intent == "show_general_catalog":
            # For general catalog requests, show featured products
            return await self.product_tool("featured", limit=self.max_products_shown)

        elif action_needed == "search_category" or intent == "search_by_category":
            category = intent_analysis.get("category")
            if category:
                return await self.product_tool("by_category", category=category, limit=self.max_products_shown)

        elif action_needed == "search_brand" or intent == "search_by_brand":
            brand = intent_analysis.get("brand")
            if brand:
                return await self.product_tool("by_brand", brand=brand, limit=self.max_products_shown)

        elif action_needed == "search_price" or intent == "search_by_price":
            price_min = intent_analysis.get("price_min")
            price_max = intent_analysis.get("price_max")
            if price_min or price_max:
                return await self.product_tool(
                    "by_price_range",
                    min_price=price_min,
                    max_price=price_max,
                    limit=self.max_products_shown,
                )

        elif action_needed == "search_products" or intent in ["search_specific_products", "get_product_details"]:
            # Use AI-extracted search terms for product search
            search_terms = intent_analysis.get("search_terms", [])
            specific_product = intent_analysis.get("specific_product")

            if specific_product:
                return await self.product_tool("search", search_term=specific_product, limit=self.max_products_shown)
            elif search_terms:
                search_term = " ".join(search_terms)
                return await self.product_tool("search", search_term=search_term, limit=self.max_products_shown)

        # Check for special preferences
        if intent_analysis.get("wants_featured"):
            return await self.product_tool("featured", limit=self.max_products_shown)

        if intent_analysis.get("wants_sale"):
            return await self.product_tool("on_sale", limit=self.max_products_shown)

        # Fallback: use advanced search with all available parameters
        search_params = {}

        if intent_analysis.get("search_terms"):
            search_params["search_term"] = " ".join(intent_analysis["search_terms"])

        if intent_analysis.get("category"):
            search_params["category"] = intent_analysis["category"]

        if intent_analysis.get("brand"):
            search_params["brand"] = intent_analysis["brand"]

        if intent_analysis.get("price_min"):
            search_params["min_price"] = intent_analysis["price_min"]

        if intent_analysis.get("price_max"):
            search_params["max_price"] = intent_analysis["price_max"]

        search_params["in_stock"] = True
        search_params["limit"] = self.max_products_shown

        # If no specific search parameters, show featured products
        if not any(search_params.get(key) for key in ["search_term", "category", "brand", "min_price", "max_price"]):
            return await self.product_tool("featured", limit=self.max_products_shown)

        return await self.product_tool("advanced_search", **search_params)

    async def _query_products_from_pgvector(self, user_query: str, intent_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Query products from PostgreSQL using pgvector semantic search.

        Args:
            user_query: Original user query
            intent_analysis: AI-analyzed user intent

        Returns:
            Dict with success, products, and metadata
        """
        try:
            # Construct semantic search query
            search_terms = intent_analysis.get("search_terms", [])
            category = intent_analysis.get("category")
            brand = intent_analysis.get("brand")
            specific_product = intent_analysis.get("specific_product")

            # Build comprehensive search query
            query_parts = []

            if specific_product:
                query_parts.append(f"product: {specific_product}")
            elif search_terms:
                query_parts.append(" ".join(search_terms))

            if category:
                query_parts.append(f"category: {category}")

            if brand:
                query_parts.append(f"brand: {brand}")

            # Fallback to original user query if no specific terms
            semantic_query = " ".join(query_parts) if query_parts else user_query

            self.logger.debug(f"pgvector semantic search query: '{semantic_query}'")

            # Generate query embedding
            query_embedding = await self.pgvector.generate_embedding(semantic_query)

            # Build metadata filters from intent
            metadata_filters = {
                "stock_required": intent_analysis.get("wants_stock_info", True),
            }

            # Add category filter if specified
            if category:
                # TODO: Look up category ID by name
                pass

            # Add brand filter if specified
            if brand:
                # TODO: Look up brand ID by name
                pass

            # Add price range filters if specified
            if intent_analysis.get("price_min"):
                metadata_filters["price_min"] = intent_analysis["price_min"]

            if intent_analysis.get("price_max"):
                metadata_filters["price_max"] = intent_analysis["price_max"]

            # Perform semantic search with metadata filtering
            results = await self.pgvector.search_similar_products(
                query_embedding=query_embedding,
                k=self.max_products_shown,
                metadata_filters=metadata_filters,
                min_similarity=self.pgvector_similarity_threshold,
                query_text=semantic_query,  # Pass query text for metrics tracking
            )

            if not results:
                self.logger.info("No results from pgvector semantic search")
                return {"success": True, "products": [], "source": "pgvector", "query": semantic_query}

            # Process results and extract product information
            products = []
            for product, similarity_score in results:
                product_data = {
                    "id": str(product.id),
                    "name": product.name,
                    "description": product.description or "",
                    "price": float(product.price),
                    "stock": int(product.stock),
                    "sku": product.sku or "",
                    "category": {
                        "name": product.category.name if product.category else "",
                        "display_name": product.category.display_name if product.category else "",
                    },
                    "brand": {"name": product.brand.name if product.brand else ""} if product.brand else None,
                    "specs": product.specs or "",
                    "similarity_score": float(similarity_score),
                    "model": product.model or "",
                }

                products.append(product_data)

            self.logger.info(
                f"pgvector found {len(products)} relevant products (similarity >= {self.pgvector_similarity_threshold})"
            )

            return {
                "success": True,
                "products": products,
                "source": "pgvector",
                "query": semantic_query,
                "total_results": len(results),
                "filtered_results": len(products),
                "avg_similarity": sum(p["similarity_score"] for p in products) / len(products) if products else 0,
            }

        except Exception as e:
            self.logger.error(f"Error querying pgvector: {str(e)}")
            return {"success": False, "error": str(e), "products": [], "source": "pgvector"}

    async def _query_products_from_chroma(self, user_query: str, intent_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Query products from ChromaDB using semantic search.

        Args:
            user_query: Original user query
            intent_analysis: AI-analyzed user intent

        Returns:
            Dict with success, products, and metadata
        """
        try:
            # Construct semantic search query in English for better embedding matching
            search_terms = intent_analysis.get("search_terms", [])
            category = intent_analysis.get("category")
            brand = intent_analysis.get("brand")
            specific_product = intent_analysis.get("specific_product")

            # Build comprehensive search query
            query_parts = []

            if specific_product:
                query_parts.append(f"product: {specific_product}")
            elif search_terms:
                query_parts.append(" ".join(search_terms))

            if category:
                query_parts.append(f"category: {category}")

            if brand:
                query_parts.append(f"brand: {brand}")

            # Fallback to original user query if no specific terms
            semantic_query = " ".join(query_parts) if query_parts else user_query

            self.logger.debug(f"ChromaDB semantic search query: '{semantic_query}'")

            # Perform semantic search (returns List[Tuple[Document, float]] when include_scores=True)
            results = await self.chroma.search_similar(
                collection_name=self.chroma_collection,
                query=semantic_query,
                k=10,  # Get more results to filter
                include_scores=True,
            )

            # Cast to correct type since we know include_scores=True returns tuples
            results = cast(List[Tuple[Document, float]], results)

            if not results:
                self.logger.info("No results from ChromaDB semantic search")
                return {"success": True, "products": [], "source": "chroma", "query": semantic_query}

            # Process results and extract product information
            products = []
            for result in results:
                try:
                    # Unpack the tuple (Document, score)
                    if isinstance(result, tuple) and len(result) == 2:
                        doc, score = result
                    else:
                        # Fallback if structure is unexpected
                        self.logger.warning(f"Unexpected result structure: {type(result)}")
                        continue

                    # Filter by similarity threshold
                    if score < self.similarity_threshold:
                        continue

                    # Parse metadata to reconstruct product data
                    metadata = doc.metadata
                    product_data = {
                        "id": metadata.get("product_id"),
                        "name": metadata.get("name", ""),
                        "description": metadata.get("description", ""),
                        "price": float(metadata.get("price", 0)),
                        "stock": int(metadata.get("stock", 0)),
                        "sku": metadata.get("sku", ""),
                        "category": {
                            "name": metadata.get("category_name", ""),
                            "display_name": metadata.get("category_display_name", ""),
                        },
                        "brand": {"name": metadata.get("brand_name", "")} if metadata.get("brand_name") else None,
                        "specs": metadata.get("specs", ""),
                        "similarity_score": float(score),
                    }

                    products.append(product_data)

                except Exception as e:
                    self.logger.warning(f"Error processing ChromaDB result: {e}")
                    continue

            # Sort by similarity score (descending)
            products.sort(key=lambda x: x.get("similarity_score", 0), reverse=True)

            # Limit to max products shown
            products = products[: self.max_products_shown]

            self.logger.info(
                f"ChromaDB found {len(products)} relevant products (similarity >= {self.similarity_threshold})"
            )

            return {
                "success": True,
                "products": products,
                "source": "chroma",
                "query": semantic_query,
                "total_results": len(results),
                "filtered_results": len(products),
            }

        except Exception as e:
            self.logger.error(f"Error querying ChromaDB: {str(e)}")
            return {"success": False, "error": str(e), "products": [], "source": "chroma"}

    async def _generate_pgvector_ai_response(
        self,
        products: List[Dict[str, Any]],
        message: str,
        _: Dict[str, Any],
        pgvector_metadata: Dict[str, Any],
    ) -> str:
        """Generate AI-powered response specifically for pgvector semantic search results."""
        # Prepare product information for AI in structured format
        product_info = []
        for product in products:
            info = {
                "name": product["name"],
                "brand": product.get("brand", {}).get("name", "N/A") if product.get("brand") else "N/A",
                "price": f"${product['price']:.2f}",
                "stock": "Available" if product["stock"] > 0 else "Out of stock",
                "stock_count": product["stock"],
                "category": product.get("category", {}).get("display_name", "N/A"),
                "description": product.get("description", "")[:150] + "..."
                if len(product.get("description", "")) > 150
                else product.get("description", ""),
                "specs": product.get("specs", "")[:100] + "..."
                if len(product.get("specs", "")) > 100
                else product.get("specs", ""),
                "similarity_score": product.get("similarity_score", 0),
                "sku": product.get("sku", ""),
            }
            product_info.append(info)

        # Create structured product data for the prompt
        products_json = []
        for info in product_info:
            products_json.append(f"""{{
    "name": "{info["name"]}",
    "brand": "{info["brand"]}",
    "price": "{info["price"]}",
    "stock": "{info["stock"]} ({info["stock_count"]} units)",
    "category": "{info["category"]}",
    "description": "{info["description"]}",
    "relevance": {info["similarity_score"]:.3f}
}}""")

        products_text = ",\n".join(products_json)

        # Enhanced English prompt for better multilingual understanding
        prompt = f"""# E-COMMERCE PRODUCT SEARCH ASSISTANT (pgvector)

## USER QUERY
"{message}"

## LANGUAGE DETECTION & RESPONSE REQUIREMENT
IMPORTANT: Detect the language of the user's query and respond in the SAME language.
- If Spanish â†’ Respond in Spanish
- If English â†’ Respond in English
- If Portuguese â†’ Respond in Portuguese
- Default: Spanish

## PGVECTOR SEMANTIC SEARCH RESULTS
Found {len(products)} highly relevant products from PostgreSQL vector search:
[
{products_text}
]

## SEARCH METADATA
- Source: PostgreSQL pgvector (native vector similarity)
- Query processed: "{pgvector_metadata.get("query", message)}"
- Total results found: {pgvector_metadata.get("total_results", 0)}
- Average similarity: {pgvector_metadata.get("avg_similarity", 0):.3f}
- Minimum threshold: {self.pgvector_similarity_threshold}

## INSTRUCTIONS
You are a helpful multilingual e-commerce assistant. Based on the high-quality semantic search results:

1. **ANALYZE LANGUAGE**: Detect user's language and respond accordingly
2. **ANALYZE RELEVANCE**: Review the similarity scores (>0.7 = excellent match)
3. **PRIORITIZE RESULTS**: Focus on products with highest relevance scores
4. **GENERATE RESPONSE**: Create a natural, conversational response that:
   - Highlights the most relevant products (top 3-5)
   - Includes key details: name, brand, price, stock status
   - Uses natural language and friendly tone
   - Includes relevant emojis moderately (1-3 max)
   - Mentions category if helpful for context
   - Suggests alternatives if needed

## RESPONSE FORMAT
- Maximum 6 lines
- Start with products found confirmation
- List top products with essential details
- End with helpful offer for more information

## QUALITY REQUIREMENTS
- **Language Match**: Respond in user's detected language
- **Accuracy**: All prices and product details must be exact
- **Relevance**: Prioritize higher similarity scores
- **Completeness**: Include stock status and key specifications
- **Tone**: Professional but friendly and helpful
- **Brevity**: Concise but informative

Generate your response now:"""

        try:
            # Use fast model for user-facing responses
            llm = self.ollama.get_llm(temperature=0.7, model="llama3.2:1b")
            response = await llm.ainvoke(prompt)
            return response.content.strip()  # type: ignore
        except Exception as e:
            self.logger.error(f"Error generating pgvector AI response: {str(e)}")
            # Fallback to formatted response
            return self._format_pgvector_fallback_response(products, pgvector_metadata)

    def _format_pgvector_fallback_response(self, products: List[Dict[str, Any]], _: Dict[str, Any]) -> str:
        """Fallback response formatting for pgvector results when AI fails."""
        if not products:
            return "ðŸ” BusquÃ© en nuestro catÃ¡logo pero no encontrÃ© productos que coincidan con tu consulta.\
                Â¿PodrÃ­as ser mÃ¡s especÃ­fico?"

        count = len(products)
        response = f"ðŸŽ¯ EncontrÃ© {count} producto{'s' if count > 1 else ''} muy relevante{'s' if count > 1 else ''}\
            para tu bÃºsqueda:\n\n"

        for i, product in enumerate(products[:5], 1):
            name = product["name"]
            brand = product.get("brand", {}).get("name", "") if product.get("brand") else ""
            price = product["price"]
            stock = product["stock"]
            similarity = product.get("similarity_score", 0)

            response += f"{i}. **{name}**"
            if brand:
                response += f" ({brand})"
            response += f" - ${price:,.2f}"

            if stock > 0:
                response += f" âœ… ({stock} disponibles)"
            else:
                response += " âŒ Sin stock"

            if similarity >= 0.8:
                response += " â­ Coincidencia exacta"

            response += "\n"

        response += "\nÂ¿Te interesa alguno? Puedo darte mÃ¡s detalles. ðŸ›’"
        return response

    async def _generate_chroma_ai_response(
        self,
        products: List[Dict[str, Any]],
        message: str,
        _: Dict[str, Any],
        chroma_metadata: Dict[str, Any],
    ) -> str:
        """Generate AI-powered response specifically for ChromaDB semantic search results."""
        # Prepare product information for AI in structured format
        product_info = []
        for product in products:
            info = {
                "name": product["name"],
                "brand": product.get("brand", {}).get("name", "N/A") if product.get("brand") else "N/A",
                "price": f"${product['price']:.2f}",
                "stock": "Available" if product["stock"] > 0 else "Out of stock",
                "stock_count": product["stock"],
                "category": product.get("category", {}).get("display_name", "N/A"),
                "description": product.get("description", "")[:150] + "..."
                if len(product.get("description", "")) > 150
                else product.get("description", ""),
                "specs": product.get("specs", "")[:100] + "..."
                if len(product.get("specs", "")) > 100
                else product.get("specs", ""),
                "similarity_score": product.get("similarity_score", 0),
                "sku": product.get("sku", ""),
            }
            product_info.append(info)

        # Create structured product data for the prompt
        products_json = []
        for info in product_info:
            products_json.append(f"""{{
    "name": "{info["name"]}",
    "brand": "{info["brand"]}",
    "price": "{info["price"]}",
    "stock": "{info["stock"]} ({info["stock_count"]} units)",
    "category": "{info["category"]}",
    "description": "{info["description"]}",
    "relevance": {info["similarity_score"]:.3f}
}}""")

        products_text = ",\n".join(products_json)

        # Enhanced English prompt for better multilingual understanding and processing
        prompt = f"""# E-COMMERCE PRODUCT SEARCH ASSISTANT

## USER QUERY
"{message}"

## LANGUAGE DETECTION & RESPONSE REQUIREMENT
IMPORTANT: Detect the language of the user's query and respond in the SAME language.
- If Spanish â†’ Respond in Spanish
- If English â†’ Respond in English  
- If Portuguese â†’ Respond in Portuguese
- Default: Spanish

## SEMANTIC SEARCH RESULTS
Found {len(products)} relevant products from vector database:
[
{products_text}
]

## SEARCH METADATA
- Source: ChromaDB semantic search
- Query processed: "{chroma_metadata.get("query", message)}"
- Total results found: {chroma_metadata.get("total_results", 0)}
- Results after filtering: {chroma_metadata.get("filtered_results", len(products))}

## INSTRUCTIONS
You are a helpful multilingual e-commerce assistant. Based on the semantic search results above:

1. **ANALYZE LANGUAGE**: Detect user's language and respond accordingly
2. **ANALYZE RELEVANCE**: Review the similarity scores and product details
3. **PRIORITIZE RESULTS**: Focus on products with higher relevance scores (>0.7 excellent, >0.5 good)
4. **GENERATE RESPONSE**: Create a natural, conversational response that:
   - Highlights the most relevant products (top 3-5)
   - Includes key details: name, brand, price, stock status
   - Uses natural language and friendly tone
   - Includes relevant emojis moderately (1-3 max)
   - Mentions category if helpful for context
   - Suggests alternatives if needed

## RESPONSE FORMAT
- Maximum 6 lines
- Start with products found confirmation
- List top products with essential details
- End with helpful offer for more information

## QUALITY REQUIREMENTS
- **Language Match**: Respond in user's detected language
- **Accuracy**: All prices and product details must be exact
- **Relevance**: Prioritize higher similarity scores
- **Completeness**: Include stock status and key specifications
- **Tone**: Professional but friendly and helpful
- **Brevity**: Concise but informative

Generate your response now:"""

        try:
            # Use fast model for user-facing responses
            llm = self.ollama.get_llm(temperature=0.7, model="llama3.2:1b")
            response = await llm.ainvoke(prompt)
            return response.content.strip()  # type: ignore
        except Exception as e:
            self.logger.error(f"Error generating ChromaDB AI response: {str(e)}")
            # Fallback to formatted response
            return self._format_chroma_fallback_response(products, chroma_metadata)

    def _format_chroma_fallback_response(self, products: List[Dict[str, Any]], _: Dict[str, Any]) -> str:
        """Fallback response formatting for ChromaDB results when AI fails."""
        if not products:
            return "ðŸ” BusquÃ© en nuestro catÃ¡logo pero no encontrÃ© productos que coincidan exactamente con tu consulta.\
                Â¿PodrÃ­as ser mÃ¡s especÃ­fico?"

        count = len(products)
        response = f"ðŸŽ¯ EncontrÃ© {count} producto{'s' if count > 1 else ''} relevante{'s' if count > 1 else ''}\
            para tu bÃºsqueda:\n\n"

        for i, product in enumerate(products[:5], 1):
            name = product["name"]
            brand = product.get("brand", {}).get("name", "") if product.get("brand") else ""
            price = product["price"]
            stock = product["stock"]

            response += f"{i}. **{name}**"
            if brand:
                response += f" ({brand})"
            response += f" - ${price:,.2f}"

            if stock > 0:
                response += f" âœ… ({stock} disponibles)"
            else:
                response += " âŒ Sin stock"
            response += "\n"

        response += "\nÂ¿Te interesa alguno? Puedo darte mÃ¡s detalles. ðŸ›’"
        return response

    async def _generate_ai_response(
        self, products: List[Dict[str, Any]], message: str, intent_analysis: Dict[str, Any]
    ) -> str:
        """Generate AI-powered response based on products and user intent."""
        # Prepare product information for AI
        product_info = []
        for product in products[: self.max_products_shown]:
            info = f"- {product['name']}"
            if product.get("brand", {}).get("name"):
                info += f" ({product['brand']['name']})"

            if self.show_prices:
                info += f" - ${product['price']:.2f}"

            if self.show_stock:
                stock = product["stock"]
                if stock > 0:
                    info += f" âœ… ({stock} en stock)"
                else:
                    info += " âŒ (Sin stock)"

            if product.get("category", {}).get("display_name"):
                info += f" | CategorÃ­a: {product['category']['display_name']}"

            if product.get("description"):
                desc_text = product["description"]
                desc = desc_text[:100] + "..." if len(desc_text) > 100 else desc_text
                info += f" | {desc}"

            product_info.append(info)

        prompt = f"""# USER QUERY
"{message}"

# LANGUAGE DETECTION
IMPORTANT: Detect the language of the user's query and respond in the SAME language.
- Spanish query â†’ Respond in Spanish
- English query â†’ Respond in English
- Portuguese query â†’ Respond in Portuguese
- Default â†’ Spanish

# SEARCH RESULTS
Found {len(products)} relevant products. Here's a summary of the main ones:
{chr(10).join(product_info[:5])}

# INSTRUCTIONS
Generate a brief response highlighting products, prices and stock availability:
- Be clear and friendly
- Maximum 5 lines
- Use emojis moderately (1-3 max)
- Match the user's language
- Include key product details (name, price, stock status)

Generate your response now:"""

        try:
            # Use fast model for user-facing responses
            llm = self.ollama.get_llm(temperature=0.7, model="llama3.2:1b")
            response = await llm.ainvoke(prompt)
            return response.content  # type: ignore
        except Exception as e:
            logger.error(f"Error generating AI product response: {str(e)}")
            # Fallback to formatted response
            return self._generate_fallback_response(products, intent_analysis)

    async def _generate_no_results_response(self, message: str, intent_analysis: Dict[str, Any]) -> str:
        """Generate response when no products are found."""
        prompt = f"""# USER QUERY
User searched for: "{message}"

# INSTRUCTIONS
No matches found. Suggest 2 relevant alternatives.
- Maximum 3 lines.
- Be cordial and helpful.
- Respond in Spanish.
"""

        try:
            # Use fast model for user-facing responses
            llm = self.ollama.get_llm(temperature=0.7, model="llama3.2:1b")
            response = await llm.ainvoke(prompt)
            return response.content  # type: ignore
        except Exception as e:
            logger.error(f"Error generating no results response: {str(e)}")
            return await self._generate_fallback_no_results(message, intent_analysis)

    def _generate_fallback_response(self, products: List[Dict[str, Any]], intent_analysis: Dict[str, Any]) -> str:
        """Generate fallback response without AI."""
        print("Generating fallback response", intent_analysis)
        if len(products) == 1:
            return self._format_single_product(products[0])
        else:
            return self._format_multiple_products(products)

    async def _generate_fallback_no_results(self, message: str, intent_analysis: Dict[str, Any]) -> str:
        """Generate AI-powered fallback no results response."""
        intent_str = json.dumps(intent_analysis, indent=2)
        prompt = f"""# USER SEARCH
User searched for: "{message}"

# INTENT ANALYSIS
{intent_str}

# INSTRUCTIONS
No matching products found. Generate a helpful response that:
- Is empathetic and understanding
- Offers 2-3 specific alternatives based on the query
- Uses a conversational and friendly tone
- Maximum 4 lines
- Responds in Spanish
- Include relevant emojis"""

        try:
            # Use fast model for user-facing responses
            llm = self.ollama.get_llm(temperature=0.7, model="llama3.2:1b")
            response = await llm.ainvoke(prompt)
            return response.content.strip()  # type: ignore
        except Exception as e:
            logger.error(f"Error generating no results response: {str(e)}")
            return f"Lo siento, no encontrÃ© productos para '{message}'. Â¿Te puedo ayudar con otra bÃºsqueda? ðŸ¤”"

    def _format_single_product(self, product: Dict[str, Any]) -> str:
        """Format response for a single product."""
        name = product.get("name", "Producto")
        price = product.get("price", 0)
        stock = product.get("stock", 0) if self.show_stock else None
        description = product.get("description", "")
        brand = product.get("brand", {}).get("name", "")
        category = product.get("category", {}).get("display_name", "")

        response = f"ðŸ“± **{name}**"
        if brand:
            response += f" ({brand})"
        response += "\n"

        if self.show_prices and price:
            response += f"ðŸ’° Precio: ${price:,.2f}\n"

        if stock is not None:
            if stock > 0:
                response += f"âœ… En stock ({stock} disponibles)\n"
            else:
                response += "âŒ Sin stock\n"

        if category:
            response += f"ðŸ“‚ CategorÃ­a: {category}\n"

        if description:
            response += f"\n{description}\n"

        response += "\nÂ¿Te interesa este producto? Â¿Necesitas mÃ¡s informaciÃ³n?"

        return response

    def _format_multiple_products(self, products: List[Dict[str, Any]]) -> str:
        """Format response for multiple products."""
        response = f"EncontrÃ© {len(products)} productos que podrÃ­an interesarte:\n\n"

        for i, product in enumerate(products, 1):
            name = product.get("name", f"Producto {i}")
            price = product.get("price", 0)
            stock = product.get("stock", 0) if self.show_stock else None
            brand = product.get("brand", {}).get("name", "")

            response += f"{i}. **{name}**"
            if brand:
                response += f" ({brand})"

            if self.show_prices and price:
                response += f" - ${price:,.2f}"

            if stock is not None:
                if stock > 0:
                    response += " âœ…"
                else:
                    response += " âŒ"

            response += "\n"

        response += "\nÂ¿Te interesa alguno en particular? Puedo darte mÃ¡s detalles."

        return response

    async def _try_catalog_response(
        self, message: str, intent_analysis: Dict[str, Any], products: List[Dict[str, Any]], state_dict: Dict[str, Any]
    ) -> Tuple[str, bool]:
        """
        Try to send WhatsApp catalog response if service is available and appropriate

        Args:
            message: User's message
            intent_analysis: AI analysis of intent
            products: Found products
            state_dict: Current conversation state

        Returns:
            Tuple[str, bool]: (response_text, catalog_sent)
        """
        catalog_sent = False
        response_text = ""

        # Check if catalog service is available and user phone is available
        if not self.catalog_service:
            return "", False

        # Get user phone from state
        user_phone = state_dict.get("user_phone") or state_dict.get("from_phone")
        if not user_phone:
            self.logger.debug("No user phone available for catalog response")
            return "", False

        try:
            # Attempt smart catalog response
            catalog_response = await self.catalog_service.send_smart_product_response(
                user_phone=user_phone, user_message=message, intent_analysis=intent_analysis, local_products=products
            )

            if catalog_response.success:
                # Catalog sent successfully
                catalog_sent = True
                response_text = self._generate_catalog_confirmation_text(len(products), intent_analysis)
                self.logger.info(f"WhatsApp catalog sent successfully to {user_phone}")

            else:
                # Catalog service decided not to send catalog or failed
                fallback_data = (
                    catalog_response.data if catalog_response.data and isinstance(catalog_response.data, dict) else {}
                )

                if fallback_data.get("fallback_to_text"):
                    # Service recommends text fallback
                    reason = fallback_data.get("reason", "Unknown reason")
                    self.logger.debug(f"Catalog service fallback: {reason}")
                    catalog_sent = False
                    response_text = ""
                else:
                    # Actual error occurred
                    self.logger.warning(f"Catalog service error: {catalog_response.error}")
                    catalog_sent = False
                    response_text = ""

        except Exception as e:
            self.logger.error(f"Error trying catalog response: {str(e)}")
            catalog_sent = False
            response_text = ""

        return response_text, catalog_sent

    def _generate_catalog_confirmation_text(self, product_count: int, intent_analysis: Dict[str, Any]) -> str:
        """
        Generate confirmation text when catalog is sent

        Args:
            product_count: Number of products in catalog
            intent_analysis: User intent analysis

        Returns:
            Confirmation message text
        """
        intent_type = intent_analysis.get("intent", "").lower()
        category = intent_analysis.get("category", "")

        if "category" in intent_type and category:
            return (
                f"ðŸ“± Te enviÃ© nuestro catÃ¡logo interactivo con {product_count} productos "
                f"de {category}. Â¡Puedes ver detalles, precios y hacer tu pedido directamente!"
            )

        elif "search" in intent_type:
            search_terms = intent_analysis.get("search_terms", [])
            if search_terms:
                terms_text = ", ".join(search_terms[:3])  # Limit to first 3 terms
                return (
                    f"ðŸ” EncontrÃ© {product_count} productos relacionados con '{terms_text}'. "
                    f"Te enviÃ© el catÃ¡logo interactivo para que puedas explorar las opciones."
                )

        return (
            f"ðŸ›ï¸ Te enviÃ© nuestro catÃ¡logo interactivo con {product_count} productos disponibles. "
            f"Puedes ver detalles, comparar precios y elegir lo que mÃ¡s te guste."
        )

    def get_catalog_status(self) -> Dict[str, Any]:
        """
        Get status of WhatsApp catalog integration

        Returns:
            Dictionary with catalog status information
        """
        return {
            "catalog_enabled": self.enable_whatsapp_catalog,
            "catalog_service_available": self.catalog_service is not None,
            "catalog_info": self.catalog_service.get_catalog_info() if self.catalog_service else None,
        }
