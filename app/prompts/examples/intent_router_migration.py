"""
Ejemplo de migraci√≥n del IntentRouter al sistema de prompts centralizado.

Este archivo muestra c√≥mo refactorizar el c√≥digo existente.
"""

from typing import Any, Dict, Optional

from app.prompts import PromptManager, PromptRegistry


class IntentRouterRefactored:
    """
    Versi√≥n refactorizada del IntentRouter usando PromptManager.

    ANTES: Prompts hardcodeados en el c√≥digo
    DESPU√âS: Prompts centralizados y configurables
    """

    def __init__(self, ollama=None, config: Optional[Dict[str, Any]] = None):
        self.ollama = ollama
        self.config = config or {}

        # ===== NUEVO: Usar PromptManager =====
        self.prompt_manager = PromptManager(
            cache_size=self.config.get("prompt_cache_size", 500), cache_ttl=self.config.get("prompt_cache_ttl", 3600)
        )

        # El resto de la configuraci√≥n permanece igual
        self.confidence_threshold = self.config.get("confidence_threshold", 0.75)
        self.fallback_agent = self.config.get("fallback_agent", "support_agent")

    async def analyze_intent_with_llm(self, message: str, state_dict: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analiza intenci√≥n usando LLM.

        CAMBIOS PRINCIPALES:
        1. Prompts cargados desde PromptManager
        2. Variables extra√≠das a diccionario
        3. Templates renderizados autom√°ticamente
        """

        # ===== ANTES =====
        # system_prompt = """
        # You are an expert intent classifier...
        # """
        #
        # user_prompt = f"""
        # ### Customer Data
        # {customer_data}
        # ...
        # """

        # ===== DESPU√âS =====
        try:
            # 1. Obtener system prompt desde el manager
            system_prompt = await self.prompt_manager.get_prompt(PromptRegistry.INTENT_ANALYZER_SYSTEM)

            # 2. Preparar variables para el user prompt
            variables = {
                "customer_data": self._format_customer_data(state_dict.get("customer_data")),
                "context_info": self._format_context_info(state_dict.get("conversation_data")),
                "message": message,
            }

            # 3. Obtener user prompt renderizado
            user_prompt = await self.prompt_manager.get_prompt(PromptRegistry.INTENT_ANALYZER_USER, variables=variables)

            # 4. Llamar a Ollama (sin cambios)
            response_text = await self.ollama.generate_response(
                system_prompt=system_prompt, user_prompt=user_prompt, temperature=0.5
            )

            # 5. Procesar respuesta (sin cambios)
            # ... resto del c√≥digo igual ...

            return {"success": True}

        except Exception as e:
            print(f"Error in LLM analysis: {e}")
            return {"success": False, "error": str(e)}

    def _format_customer_data(self, customer_data: Optional[Dict[str, Any]]) -> str:
        """Helper para formatear datos del cliente."""
        if not customer_data:
            return "No hay datos de cliente disponibles"

        import json

        return json.dumps(customer_data, indent=2)

    def _format_context_info(self, conversation_data: Optional[Dict[str, Any]]) -> str:
        """Helper para formatear informaci√≥n de contexto."""
        if not conversation_data:
            return "No hay contexto de conversaci√≥n"

        context_parts = []
        if channel := conversation_data.get("channel"):
            context_parts.append(f"Channel: {channel}")
        if language := conversation_data.get("language"):
            context_parts.append(f"Language: {language}")

        return ", ".join(context_parts) if context_parts else "Sin contexto adicional"


# ===== EJEMPLO DE USO =====


async def compare_approaches():
    """Compara el approach antiguo vs el nuevo."""

    print("=" * 70)
    print("COMPARACI√ìN: ANTES vs DESPU√âS")
    print("=" * 70)

    # Setup
    message = "Quiero comprar una laptop gamer"
    state_dict = {
        "customer_data": {"name": "Juan", "tier": "VIP"},
        "conversation_data": {"channel": "whatsapp", "language": "es"},
    }

    print("\nüìã ANTES - Prompt Hardcodeado:")
    print("-" * 70)
    print("""
    def analyze_intent_with_llm(message, state_dict):
        # ‚ùå Prompt hardcodeado en el c√≥digo
        system_prompt = '''
        You are an expert intent classifier for an e-commerce assistant.
        Your task is to analyze the context and user message...
        '''

        # ‚ùå Dif√≠cil de mantener
        # ‚ùå No versionado
        # ‚ùå No cacheable
        # ‚ùå Requiere redeploy para cambios

        user_prompt = f'''
        ### Customer Data
        {json.dumps(customer_data, indent=2)}

        ### Current User Message
        "{message}"
        '''

        response = await ollama.generate_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt
        )
        return response
    """)

    print("\n‚úÖ DESPU√âS - Sistema Centralizado:")
    print("-" * 70)
    print("""
    def analyze_intent_with_llm(message, state_dict):
        # ‚úÖ Prompts centralizados
        system_prompt = await self.prompt_manager.get_prompt(
            PromptRegistry.INTENT_ANALYZER_SYSTEM
        )

        # ‚úÖ Variables estructuradas
        variables = {
            "customer_data": self._format_customer_data(state_dict.get("customer_data")),
            "context_info": self._format_context_info(state_dict.get("conversation_data")),
            "message": message
        }

        # ‚úÖ Renderizado autom√°tico
        user_prompt = await self.prompt_manager.get_prompt(
            PromptRegistry.INTENT_ANALYZER_USER,
            variables=variables
        )

        # ‚úÖ Mantenible: editar YAML sin tocar c√≥digo
        # ‚úÖ Versionado: historial completo de cambios
        # ‚úÖ Cacheable: performance mejorada
        # ‚úÖ Din√°mico: cambios sin redeploy

        response = await ollama.generate_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt
        )
        return response
    """)

    print("\nüìä BENEFICIOS:")
    print("-" * 70)
    benefits = [
        ("Mantenibilidad", "Prompts en YAML, f√°ciles de editar"),
        ("Versionado", "Historial completo, rollback posible"),
        ("Performance", "Cach√© inteligente reduce latencia"),
        ("Flexibilidad", "Prompts din√°micos sin redeploy"),
        ("Colaboraci√≥n", "Equipo puede editar prompts"),
        ("Testing", "A/B testing de diferentes versiones"),
        ("Auditor√≠a", "Tracking de cambios y performance"),
        ("Type-Safety", "Registry con autocompletado"),
    ]

    for benefit, description in benefits:
        print(f"  ‚úÖ {benefit:15s} ‚Üí {description}")


# ===== GU√çA DE MIGRACI√ìN PASO A PASO =====


def print_migration_guide():
    """Imprime gu√≠a de migraci√≥n."""
    print("\n\n" + "=" * 70)
    print("GU√çA DE MIGRACI√ìN PASO A PASO")
    print("=" * 70)

    steps = [
        (
            "1. Instalar PromptManager",
            """
        from app.prompts import PromptManager, PromptRegistry

        class YourService:
            def __init__(self):
                self.prompt_manager = PromptManager()
        """,
        ),
        (
            "2. Identificar prompts hardcodeados",
            """
        # Buscar en tu c√≥digo:
        - Strings multil√≠nea con f-strings
        - Variables tipo "system_prompt", "user_prompt"
        - Prompts largos (>100 caracteres)
        - Prompts con l√≥gica de templating manual
        """,
        ),
        (
            "3. Extraer a variables",
            """
        # Crear diccionario de variables
        variables = {
            "message": message,
            "context": context,
            "user_data": user_data
        }
        """,
        ),
        (
            "4. Usar PromptManager",
            """
        # Reemplazar prompt hardcodeado
        system_prompt = await self.prompt_manager.get_prompt(
            PromptRegistry.YOUR_PROMPT_KEY,
            variables=variables
        )
        """,
        ),
        (
            "5. Verificar y testear",
            """
        # Asegurarse de que:
        - Variables coincidan con template
        - Prompts se carguen correctamente
        - Funcionalidad no cambia
        - Performance mejora con cach√©
        """,
        ),
    ]

    for step, code in steps:
        print(f"\n{step}")
        print("-" * 70)
        print(code)


if __name__ == "__main__":
    import asyncio

    async def main():
        await compare_approaches()
        print_migration_guide()

        print("\n\n" + "=" * 70)
        print("¬°MIGRACI√ìN COMPLETA!")
        print("=" * 70)
        print("\nPr√≥ximos pasos:")
        print("1. Aplicar mismo patr√≥n a otros servicios")
        print("2. Crear prompts din√°micos para casos especiales")
        print("3. Configurar A/B testing")
        print("4. Monitorear m√©tricas de performance")

    asyncio.run(main())
